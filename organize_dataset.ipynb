{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code execution time: 33.41648507118225 sec\n",
      "4350\n",
      "[{'participant': 'H01', 'task': 'T1', 'data': [[6078, 10416, 0, 0, '10:45:37.089'], [6078, 10416, 0, 0, '10:45:37.094'], [6100, 10440, 0, 0, '10:45:37.099'], [6133, 10474, 0, 0, '10:45:37.104'], [6175, 10521, 0, 0, '10:45:37.109'], [6223, 10578, 0, 0, '10:45:37.114'], [6258, 10621, 0, 0, '10:45:37.119'], [6290, 10666, 0, 0, '10:45:37.124'], [6319, 10706, 0, 0, '10:45:37.129'], [6347, 10743, 0, 0, '10:45:37.134'], [6369, 10778, 0, 0, '10:45:37.139'], [6389, 10809, 0, 0, '10:45:37.144'], [6403, 10836, 0, 0, '10:45:37.149'], [6417, 10861, 0, 0, '10:45:37.154'], [6425, 10878, 0, 0, '10:45:37.159'], [6432, 10891, 0, 0, '10:45:37.164'], [6432, 10891, 0, 0, '10:45:37.169'], [6432, 10891, 0, 0, '10:45:37.174'], [6432, 10891, 0, 0, '10:45:37.179'], [6422, 10871, 0, 0, '10:45:37.184'], [6410, 10848, 0, 0, '10:50:21.174'], [6379, 10789, 0, 0, '10:50:21.179'], [6359, 10756, 0, 0, '10:50:21.184'], [6338, 10721, 0, 0, '10:50:21.189'], [6313, 10681, 0, 0, '10:50:21.194'], [6288, 10639, 0, 0, '10:50:21.199'], [6264, 10597, 0, 0, '10:50:21.204'], [6239, 10551, 0, 0, '10:50:21.209'], [6217, 10506, 0, 0, '10:50:21.214'], [6197, 10464, 0, 0, '10:50:21.219'], [6177, 10423, 0, 0, '10:50:21.224'], [6163, 10386, 0, 0, '10:50:21.229'], [6150, 10358, 0, 0, '10:50:21.234'], [6143, 10335, 0, 0, '10:50:21.239'], [6140, 10321, 0, 0, '10:50:21.244'], [6140, 10321, 0, 0, '10:50:21.249'], [6140, 10321, 0, 0, '10:55:05.239'], [6168, 10343, 0, 0, '10:55:05.244'], [6184, 10365, 0, 0, '10:55:05.249'], [6200, 10391, 0, 0, '10:55:05.254'], [6220, 10420, 0, 0, '10:55:05.259'], [6241, 10452, 0, 0, '10:55:05.264'], [6263, 10486, 0, 0, '10:55:05.269'], [6287, 10525, 0, 0, '10:55:05.274'], [6311, 10564, 0, 0, '10:55:05.279'], [6334, 10601, 0, 0, '10:55:05.284'], [6358, 10637, 0, 0, '10:55:05.289'], [6382, 10669, 0, 0, '10:55:05.294'], [6403, 10698, 0, 0, '10:59:49.284'], [6440, 10749, 0, 0, '10:59:49.289'], [6455, 10768, 0, 0, '10:59:49.294'], [6468, 10785, 0, 0, '10:59:49.299'], [6477, 10799, 0, 0, '10:59:49.304'], [6484, 10808, 0, 0, '10:59:49.309'], [6484, 10808, 0, 0, '10:59:49.314'], [6489, 10819, 0, 0, '10:59:49.319'], [6489, 10819, 0, 0, '10:59:49.324'], [6489, 10819, 0, 0, '10:59:49.329'], [6480, 10812, 0, 0, '10:59:49.334'], [6480, 10812, 0, 0, '10:59:49.339'], [6463, 10795, 0, 0, '10:59:49.344'], [6452, 10783, 0, 0, '10:59:49.349'], [6438, 10771, 0, 0, '10:59:49.354'], [6422, 10756, 0, 0, '10:59:49.359'], [6407, 10741, 0, 0, '10:59:49.364'], [6392, 10725, 0, 0, '10:59:49.369'], [6379, 10710, 0, 0, '10:59:49.374'], [6369, 10697, 0, 0, '10:59:49.379'], [6359, 10684, 0, 0, '10:59:49.384'], [6353, 10675, 0, 0, '11:04:33.374'], [6347, 10665, 0, 0, '11:04:33.379'], [6347, 10665, 0, 0, '11:04:33.384'], [6347, 10665, 0, 0, '11:04:33.389'], [6356, 10676, 0, 0, '11:04:33.394'], [6365, 10688, 0, 0, '11:04:33.399'], [6379, 10705, 0, 0, '11:04:33.404'], [6396, 10727, 0, 0, '11:04:33.409'], [6419, 10753, 0, 0, '11:04:33.414'], [6447, 10786, 0, 0, '11:04:33.419'], [6484, 10823, 0, 0, '11:04:33.424'], [6526, 10866, 0, 0, '11:04:33.429'], [6574, 10911, 0, 0, '11:04:33.434'], [6712, 11029, 0, 0, '11:04:33.439']]}]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define the root directory\n",
    "ROOT_DIR = \"DARWIN-RAW_experiment\" # ADJUST NAME\n",
    "\n",
    "# Function to read metadata text file\n",
    "def read_metadata(metadata_path, folder_id):\n",
    "    metadata = {\"folder_id\": folder_id}  # Include folder_id for reference\n",
    "    with open(metadata_path, \"r\", encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            key, value = line.strip().split(\": \", 1)  # Splitting \"Key: Value\" construction of metadata text files\n",
    "            metadata[key] = int(value) if value.isdigit() else value  # Convert numbers\n",
    "    return metadata\n",
    "\n",
    "# Function to read data_entries text file\n",
    "def read_data_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\" \")  # Values are separated by one space are added in a list\n",
    "            data.append([int(parts[0]), int(parts[1]), int(parts[2]), int(parts[3]), parts[4]])          #??????? We dont need to part[3] (either 0/1) given we got the part[2] (pressure) , reducing the size of the dataset\n",
    "    return data\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Lists to store metadata and data separately\n",
    "metadata_list = []\n",
    "data_entries = []\n",
    "\n",
    "# Iterations through folders and files\n",
    "for folder in sorted(os.listdir(ROOT_DIR)):  # create a list of folders which are participants\n",
    "    folder_path = os.path.join(ROOT_DIR, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        files = sorted(os.listdir(folder_path)) # create a list of files which are either writing tasks or participant metadata\n",
    "\n",
    "        # Detect metadata file first (always starts with \"anagrafica_\")\n",
    "        metadata_file = next((f for f in files if f.startswith(\"anagrafica_\")), None) # since we expect only one file we take the first file we encounter. \n",
    "        if not metadata_file:\n",
    "            continue  # Skip if no metadata file found (unlikely)\n",
    "\n",
    "        metadata_path = os.path.join(folder_path, metadata_file)\n",
    "        metadata = read_metadata(metadata_path, folder)\n",
    "        metadata_list.append(metadata)\n",
    "\n",
    "        # Read the remaining 25 data_entries files\n",
    "        for data_file in files:\n",
    "            if data_file.endswith(\".txt\") and not data_file.startswith(\"anagrafica_\"):\n",
    "                file_path = os.path.join(folder_path, data_file)\n",
    "                \n",
    "                task_name = os.path.splitext(data_file)[0]\n",
    "\n",
    "                data_entries.append({\n",
    "                    \"participant\": folder,\n",
    "                    \"task\": data_file.replace(\".txt\", \"\"),\n",
    "                    \"data\": read_data_file(file_path)\n",
    "                })\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Code execution time: {execution_time} sec\")\n",
    "\n",
    "print(len(data_entries))  # To see how many entries are in the dataset\n",
    "print(data_entries[:1])  # Check first few entries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code execution time: 70.52170395851135 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Save metadata separately as a Pandas DataFrame\n",
    "metadata_df = pd.DataFrame(metadata_list)\n",
    "metadata_df.to_csv(\"metadata.csv\", index=False, encoding=\"latin-1\")  # Save as CSV\n",
    "\n",
    "# Save data_entires in JSON\n",
    "with open(\"data_entries.json\", \"w\", encoding=\"UTF-8\") as f:\n",
    "    json.dump(data_entries, f, indent=None, ensure_ascii=True)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Code execution time: {execution_time} sec\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasciencecourses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
